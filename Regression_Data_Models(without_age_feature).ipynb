{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkBsy_9pnt3T"
      },
      "source": [
        "\n",
        "#Setting Up the Models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSH1cC5MnyGK"
      },
      "source": [
        "##Install Libraries and Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTV2QsXZmopa"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements_rg.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWLGHlwsn18N"
      },
      "outputs": [],
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from interpret.glassbox import ExplainableBoostingRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "from sklearn.metrics import  mean_squared_error, r2_score, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from interpret import show\n",
        "import shap\n",
        "import graphviz\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "from scipy.stats import spearmanr\n",
        "from collections import Counter\n",
        "\n",
        "import random\n",
        "\n",
        "# fetch dataset\n",
        "parkinsons_telemonitoring = fetch_ucirepo(id=189)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "df = parkinsons_telemonitoring.data.original.copy()\n",
        "\n",
        "SEED = 100\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZnThT8qq5w0"
      },
      "source": [
        "##Handling Outliers with IQR Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ci_wQGj8rI4D"
      },
      "outputs": [],
      "source": [
        "for col in df.columns:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "\n",
        "    df[col] = np.where(df[col] > upper, upper, df[col])\n",
        "    df[col] = np.where(df[col] < lower, lower, df[col])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpbUYBfPrMPJ"
      },
      "source": [
        "##Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7k6YoWCrPFd"
      },
      "outputs": [],
      "source": [
        "X = df.drop(['total_UPDRS', 'motor_UPDRS', 'subject#', 'age'], axis=1)\n",
        "y = df['total_UPDRS']\n",
        "\n",
        "#change column names for LightGBM\n",
        "X.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X.columns]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hmT_hpesIpB"
      },
      "source": [
        "##Creating the Optimized Models and Then Fitting them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1SsWsSrsJJ0"
      },
      "outputs": [],
      "source": [
        "dt = DecisionTreeRegressor(ccp_alpha=0.0, criterion='squared_error', max_depth=19,\n",
        "                           max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
        "                           min_samples_leaf=14, min_samples_split=5, min_weight_fraction_leaf=0.0,\n",
        "                           monotonic_cst=None, random_state=100, splitter='random')\n",
        "\n",
        "ebm = ExplainableBoostingRegressor(callback=None, cat_smooth=10.0, cyclic_progress=False,\n",
        "                                   early_stopping_rounds=100, early_stopping_tolerance=1e-05, exclude=None,\n",
        "                                   feature_names=None, feature_types=None, gain_scale=5.0,\n",
        "                                   greedy_ratio=10.0, inner_bags=0, interaction_smoothing_rounds=100,\n",
        "                                   interactions=6, learning_rate=0.0027763309052116383, max_bins=484,\n",
        "                                   max_delta_step=0.0, max_interaction_bins=263, max_leaves=28,\n",
        "                                   max_rounds=50000, min_cat_samples=10, min_hessian=0.0,\n",
        "                                   min_samples_leaf=15, missing='separate', monotone_constraints=None,\n",
        "                                   n_jobs=-2, objective='rmse', outer_bags=14,\n",
        "                                   random_state=100, reg_alpha=0.0, reg_lambda=0.0,\n",
        "                                   smoothing_rounds=500, validation_size=0.15)\n",
        "\n",
        "cat = CatBoostRegressor(iterations=816, learning_rate=0.1261192471208588, depth=10,\n",
        "                        l2_leaf_reg=9.06507208082491, loss_function='RMSE', bootstrap_type='MVS',\n",
        "                        random_state=100)\n",
        "\n",
        "lgbm = LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=0.7507121131532456,\n",
        "                     importance_type='split', learning_rate=0.042853382162136154, max_depth=18,\n",
        "                     min_child_samples=8, min_child_weight=0.001, min_split_gain=0.0,\n",
        "                     n_estimators=768, n_jobs=None, num_leaves=355,\n",
        "                     objective=None, random_state=100, reg_alpha=2.592134383897951,\n",
        "                     reg_lambda=6.6855134447038855, subsample=0.9198022636755252, subsample_for_bin=200000,\n",
        "                     subsample_freq=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVC0ePnR9XhV"
      },
      "outputs": [],
      "source": [
        "dt.fit(X_train, y_train)\n",
        "ebm.fit(X_train, y_train)\n",
        "cat.fit(X_train, y_train)\n",
        "lgbm.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_urWFNo9nRd"
      },
      "source": [
        "##Evaluating Each Model's Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLUMxZFJ9o5O"
      },
      "outputs": [],
      "source": [
        "#For dt\n",
        "dt_pred = dt.predict(X_test)\n",
        "print(\"\\nFor the Decision Tree Model:\")\n",
        "print(\"\\nMSE is : \" + str(mean_squared_error(y_test, dt_pred)))\n",
        "print(\"\\nR2 is : \" + str(r2_score(y_test, dt_pred)))\n",
        "print(\"\\nMAE is : \" + str(mean_absolute_error(y_test, dt_pred)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeA0YLLt-Bil"
      },
      "outputs": [],
      "source": [
        "#For ebm\n",
        "ebm_pred = ebm.predict(X_test)\n",
        "print(\"\\nFor the Explainable Boosting Machine Model:\")\n",
        "print(\"\\nMSE is : \" + str(mean_squared_error(y_test, ebm_pred)))\n",
        "print(\"\\nR2 is : \" + str(r2_score(y_test, ebm_pred)))\n",
        "print(\"\\nMAE is : \" + str(mean_absolute_error(y_test, ebm_pred)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRSn1FMN-X3k"
      },
      "outputs": [],
      "source": [
        "#For cat\n",
        "cat_pred = cat.predict(X_test)\n",
        "print(\"\\nFor the CatBoost Model:\")\n",
        "print(\"\\nMSE is : \" + str(mean_squared_error(y_test, cat_pred)))\n",
        "print(\"\\nR2 is : \" + str(r2_score(y_test, cat_pred)))\n",
        "print(\"\\nMAE is : \" + str(mean_absolute_error(y_test, cat_pred)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3ieg0n--hoQ"
      },
      "outputs": [],
      "source": [
        "#For lgbm\n",
        "lgbm_pred = lgbm.predict(X_test)\n",
        "print(\"\\nFor the LightGBM Model:\")\n",
        "print(\"\\nMSE is : \" + str(mean_squared_error(y_test, lgbm_pred)))\n",
        "print(\"\\nR2 is : \" + str(r2_score(y_test, lgbm_pred)))\n",
        "print(\"\\nMAE is : \" + str(mean_absolute_error(y_test, lgbm_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kTy78fW-wcc"
      },
      "source": [
        "#Global Explainability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyp4NicG-y65"
      },
      "source": [
        "##Decision Tree's Tree Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1-y-VZD-4KY"
      },
      "outputs": [],
      "source": [
        "dot_data = export_graphviz(dt, out_file=None,\n",
        "                                feature_names=X.columns,\n",
        "                                filled=True)\n",
        "\n",
        "graph = graphviz.Source(dot_data, format=\"png\")\n",
        "graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAFYwSU3_N_y"
      },
      "source": [
        "##EBM Global Explainability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8YrH9ZU_S9W"
      },
      "outputs": [],
      "source": [
        "ebm_global = ebm.explain_global()\n",
        "show(ebm_global)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKjCw2Ub_s5z"
      },
      "source": [
        "##Setting Up SHAP explainers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olxEivkI_tjD"
      },
      "outputs": [],
      "source": [
        "#turn data into DF for SHAP plots\n",
        "X_test_df = pd.DataFrame(X_test, columns=X.columns)\n",
        "\n",
        "#SHAP for dt\n",
        "dt_explainer = shap.TreeExplainer(dt)\n",
        "dt_shap_values = dt_explainer.shap_values(X_test_df)\n",
        "\n",
        "#wrapper function for EBM predict to handle feature names\n",
        "def ebm_predict_wrapper(X):\n",
        "    X_df = pd.DataFrame(X, columns=X_train.columns)\n",
        "    return ebm.predict(X_df)\n",
        "\n",
        "#SHAP for ebm\n",
        "ebm_explainer = shap.KernelExplainer(ebm_predict_wrapper, X_train, seed=SEED)\n",
        "ebm_shap_values = ebm_explainer.shap_values(X_test_df)\n",
        "\n",
        "#SHAP for cat\n",
        "cat_explainer = shap.TreeExplainer(cat)\n",
        "cat_shap_values = cat_explainer.shap_values(X_test_df)\n",
        "\n",
        "#SHAP for lgbm\n",
        "lgbm_explainer = shap.Explainer(lgbm)\n",
        "lgbm_shap_values = lgbm_explainer.shap_values(X_test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raGQlBcgAbzr"
      },
      "source": [
        "##SHAP Summary Plots For Each Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrr2w7zsAZvC"
      },
      "outputs": [],
      "source": [
        "#dt\n",
        "shap.summary_plot(dt_shap_values, X_test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EL0_LuJ8BESJ"
      },
      "outputs": [],
      "source": [
        "#ebm\n",
        "shap.summary_plot(ebm_shap_values, X_test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjR2Hf1sBHNp"
      },
      "outputs": [],
      "source": [
        "#cat\n",
        "shap.summary_plot(cat_shap_values, X_test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlRZdHC-BI9V"
      },
      "outputs": [],
      "source": [
        "#lgbm\n",
        "shap.summary_plot(lgbm_shap_values, X_test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqE486sLBqGk"
      },
      "source": [
        "#Local Explainability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am48yC0SBs5n"
      },
      "source": [
        "##Selecting Instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-v6Mu6eBvaj"
      },
      "outputs": [],
      "source": [
        "index = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9VJB0jBBxoV"
      },
      "source": [
        "##Decision Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lteq0CmOCBaS"
      },
      "outputs": [],
      "source": [
        "X_instance = X_test.iloc[[index]]\n",
        "\n",
        "node_indicator = dt.decision_path(X_instance)\n",
        "leaf_id = dt.apply(X_instance)\n",
        "\n",
        "print(f\"\\nDecision path for instance {index}:\")\n",
        "for node_id in node_indicator.indices:\n",
        "    if dt.tree_.children_left[node_id] != dt.tree_.children_right[node_id]:\n",
        "        feature = X_test.columns[dt.tree_.feature[node_id]]\n",
        "        threshold = dt.tree_.threshold[node_id]\n",
        "        if X_instance.iloc[0, dt.tree_.feature[node_id]] <= threshold:\n",
        "            threshold_sign = \"<=\"\n",
        "        else:\n",
        "            threshold_sign = \">\"\n",
        "        print(f\"  {feature} = {X_instance.iloc[0, dt.tree_.feature[node_id]]:.2f} \"\n",
        "              f\"{threshold_sign} {threshold:.2f}\")\n",
        "\n",
        "pred_value = dt.predict(X_instance)[0]\n",
        "true_value = y_test.iloc[index] if isinstance(y_test, pd.Series) else y_test[index]\n",
        "\n",
        "print(f\"\\nPredicted value: {pred_value}\")\n",
        "print(f\"Actual value: {true_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu8R0C3JCnTh"
      },
      "source": [
        "##Local EBM Explainability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7uU1UY7CuSo"
      },
      "outputs": [],
      "source": [
        "ebm_local = ebm.explain_local(X_test, y_test)\n",
        "show(ebm_local)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbJcxGZhC398"
      },
      "source": [
        "##SHAP Waterfalls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCEHmvwc6LBd"
      },
      "outputs": [],
      "source": [
        "#dt\n",
        "shap.initjs()\n",
        "shap.force_plot(dt_explainer.expected_value, dt_shap_values[index, :], X_test_df.iloc[index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKG8H_lJDDfU"
      },
      "outputs": [],
      "source": [
        "#ebm\n",
        "shap.initjs()\n",
        "shap.force_plot(ebm_explainer.expected_value, ebm_shap_values[index, :], X_test_df.iloc[index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSSVWntTDHS1"
      },
      "outputs": [],
      "source": [
        "#cat\n",
        "shap.initjs()\n",
        "shap.force_plot(cat_explainer.expected_value, cat_shap_values[index, :], X_test_df.iloc[index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7GKGyeSDLan"
      },
      "outputs": [],
      "source": [
        "#lgbm\n",
        "shap.initjs()\n",
        "shap.force_plot(lgbm_explainer.expected_value, lgbm_shap_values[index, :], X_test_df.iloc[index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaOpQJNWATQO"
      },
      "source": [
        "#SHAP Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgoFNm0pAVml"
      },
      "source": [
        "##Setting up Importances for EBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Obafo1qNAWsX"
      },
      "outputs": [],
      "source": [
        "term_names = np.array(ebm.term_names_)\n",
        "term_importances = np.array(ebm.term_importances())\n",
        "print(term_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oljb3hO5AXAH"
      },
      "outputs": [],
      "source": [
        "main_mask = np.array([' & ' not in name for name in term_names])\n",
        "main_features = term_names[main_mask]\n",
        "main_importances = term_importances[main_mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGPw4CzIAYZN"
      },
      "outputs": [],
      "source": [
        "ebm_importances = np.zeros(len(X.columns))\n",
        "for i, feature in enumerate(X.columns):\n",
        "    if feature in main_features:\n",
        "        idx = list(main_features).index(feature)\n",
        "        ebm_importances[i] = main_importances[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeUtCk_BAZ8y"
      },
      "outputs": [],
      "source": [
        "models_dict = {\n",
        "    'DT': (dt, dt_explainer, dt_shap_values, dt.feature_importances_),\n",
        "    'EBM': (ebm, ebm_explainer, ebm_shap_values, ebm_importances),\n",
        "    'CAT': (cat, cat_explainer, cat_shap_values, cat.feature_importances_),\n",
        "    'LGBM': (lgbm, lgbm_explainer, lgbm_shap_values, lgbm.feature_importances_)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsb-5yRCAbvh"
      },
      "source": [
        "##Fidelity (Correlation between model and SHAP feature importances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGA0JCyWAdJa"
      },
      "outputs": [],
      "source": [
        "def fidelity(model_importance, shap_values):\n",
        "    shap_importance = np.abs(shap_values).mean(axis=0)\n",
        "    return spearmanr(model_importance, shap_importance)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35KqVp8OAe1w"
      },
      "source": [
        "##Consistency (Entropy of top feature across instances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jV4uWmK-Ahst"
      },
      "outputs": [],
      "source": [
        "def consistency(shap_values, feature_names):\n",
        "    top_feature = np.argmax(np.abs(shap_values), axis=1)\n",
        "\n",
        "    value, counts = np.unique(top_feature, return_counts=True)\n",
        "    probs = counts / len(top_feature)\n",
        "    entropy = -np.sum(probs * np.log(probs))\n",
        "\n",
        "    dominant_feature = feature_names[value[np.argmax(counts)]]\n",
        "    dominant_percent = counts.max() / len(top_feature)\n",
        "\n",
        "    return entropy, dominant_feature, dominant_percent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fEBMpX6AieV"
      },
      "source": [
        "##Robustness (Test if SHAP values remain stable under small perturbations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcltzmbzAlCl"
      },
      "outputs": [],
      "source": [
        "def robustness(explainer, X_sample_df, n_instances=10, n_perturbations=10, noise_std=0.1, seed=100):\n",
        "    np.random.seed(seed)\n",
        "    stabilities = []\n",
        "\n",
        "    for i in range(min(n_instances, len(X_sample_df))):\n",
        "        instance_df = X_sample_df.iloc[i:i+1]\n",
        "        base_shap = explainer.shap_values(instance_df)\n",
        "\n",
        "        corrs = []\n",
        "        for _ in range(n_perturbations):\n",
        "            noise = np.random.normal(0, noise_std, instance_df.shape)\n",
        "            perturbed_df = pd.DataFrame(instance_df.values + noise, columns=instance_df.columns)\n",
        "            perturbed_shap = explainer.shap_values(perturbed_df)\n",
        "            corr = spearmanr(base_shap.flatten(), perturbed_shap.flatten())[0]\n",
        "            corrs.append(corr)\n",
        "\n",
        "        stabilities.append(np.mean(corrs))\n",
        "\n",
        "    return np.mean(stabilities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eV7aB-AAoME"
      },
      "source": [
        "##Sufficiency (Test if top-k SHAP features preserve predictions)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNT4lvrMApsZ"
      },
      "outputs": [],
      "source": [
        "def sufficiency(model, X_test, shap_values, feature_names, k=5, n_samples=30):\n",
        "    abs_errors = []\n",
        "    rel_errors = []\n",
        "    all_top_features = []\n",
        "\n",
        "    for i in range(min(n_samples, len(X_test))):\n",
        "        X_instance = X_test.iloc[i:i+1]\n",
        "        original_pred = model.predict(X_instance)[0]\n",
        "\n",
        "        shap_vals = shap_values[i, :]\n",
        "\n",
        "        top_k_indices = np.argsort(np.abs(shap_vals))[-k:]\n",
        "        all_top_features.extend(top_k_indices)\n",
        "\n",
        "        masked_instance = pd.DataFrame(np.zeros((1, X_test.shape[1])), columns=X_test.columns)\n",
        "        masked_instance.iloc[0, top_k_indices] = X_test.iloc[i, top_k_indices]\n",
        "\n",
        "        masked_pred = model.predict(masked_instance)[0]\n",
        "\n",
        "        abs_error = abs(original_pred - masked_pred)\n",
        "        rel_error = abs_error / (abs(original_pred) + 1e-10)\n",
        "\n",
        "        abs_errors.append(abs_error)\n",
        "        rel_errors.append(rel_error)\n",
        "\n",
        "    feature_counter = Counter(all_top_features)\n",
        "\n",
        "    feature_usage = []\n",
        "    for feat_idx in range(len(feature_names)):\n",
        "        count = feature_counter.get(feat_idx, 0)\n",
        "        percent = (count / n_samples) * 100\n",
        "        feature_usage.append({'feature': feature_names[feat_idx], 'count': count, 'percentage': percent})\n",
        "\n",
        "    feature_usage_df = pd.DataFrame(feature_usage).sort_values('count', ascending=False)\n",
        "\n",
        "    top_features_dict = {\n",
        "        feature_names[idx]: count\n",
        "        for idx, count in feature_counter.most_common(k)\n",
        "    }\n",
        "\n",
        "    threshold = 0.10\n",
        "    predictions_maintained = np.mean([err < threshold for err in rel_errors])\n",
        "\n",
        "    return {\n",
        "        'avg_abs_error': np.mean(abs_errors),\n",
        "        'avg_rel_error': np.mean(rel_errors),\n",
        "        'percent_maintained': predictions_maintained,\n",
        "        'top_features_used': top_features_dict,\n",
        "        'feature_usage_df': feature_usage_df\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVPnTqD9ePry"
      },
      "source": [
        "##Completeness (Test if top-k SHAP features removals preserve predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C97LuPpyeQrj"
      },
      "outputs": [],
      "source": [
        "def completeness(model, X_test, shap_values, feature_names, k=5, n_samples=30):\n",
        "    abs_errors = []\n",
        "    rel_errors = []\n",
        "    all_removed_features = []\n",
        "\n",
        "    for i in range(min(n_samples, len(X_test))):\n",
        "        X_instance = X_test.iloc[i:i+1]\n",
        "        original_pred = model.predict(X_instance)[0]\n",
        "\n",
        "        shap_vals = shap_values[i, :]\n",
        "        top_k_indices = np.argsort(np.abs(shap_vals))[-k:]\n",
        "        all_removed_features.extend(top_k_indices)\n",
        "\n",
        "        masked_instance = X_instance.copy()\n",
        "        masked_instance.iloc[0, top_k_indices] = 0\n",
        "\n",
        "        masked_pred = model.predict(masked_instance)[0]\n",
        "\n",
        "        abs_error = abs(original_pred - masked_pred)\n",
        "        rel_error = abs_error / (abs(original_pred) + 1e-10)\n",
        "\n",
        "        abs_errors.append(abs_error)\n",
        "        rel_errors.append(rel_error)\n",
        "\n",
        "    feature_counter = Counter(all_removed_features)\n",
        "\n",
        "    feature_usage = []\n",
        "    for feat_idx in range(len(feature_names)):\n",
        "        count = feature_counter.get(feat_idx, 0)\n",
        "        percent = (count / n_samples) * 100\n",
        "        feature_usage.append({'feature': feature_names[feat_idx], 'count': count, 'percentage': percent})\n",
        "\n",
        "    feature_usage_df = pd.DataFrame(feature_usage).sort_values('count', ascending=False)\n",
        "\n",
        "    top_features_dict = {feature_names[idx]: count for idx, count in feature_counter.most_common(k)}\n",
        "\n",
        "    threshold = 0.10\n",
        "    predictions_maintained = np.mean([err < threshold for err in rel_errors])\n",
        "\n",
        "    return {\n",
        "        'avg_abs_error': np.mean(abs_errors),\n",
        "        'avg_rel_error': np.mean(rel_errors),\n",
        "        'percent_maintained': predictions_maintained,\n",
        "        'top_features_removed': top_features_dict,\n",
        "        'feature_usage_df': feature_usage_df\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pisbyzclArtS"
      },
      "source": [
        "##SHAP Evaluation Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuTY65BJAs24"
      },
      "outputs": [],
      "source": [
        "results = {}\n",
        "\n",
        "for model_name, (model, explainer, shap_vals, feat_imp) in models_dict.items():\n",
        "    fl_score = fidelity(feat_imp, shap_vals)\n",
        "\n",
        "    cs_entropy, dominant_feature, dominant_percent = consistency(shap_vals, X.columns)\n",
        "    cs_score = 1 - (cs_entropy / np.log(len(X.columns)))#min-max normalization\n",
        "\n",
        "    rb_baseline = robustness(explainer, X_test_df.head(10), seed=SEED)\n",
        "    rb_more_instances = robustness(explainer, X_test_df.head(30), n_instances=30, seed=SEED)\n",
        "    rb_more_perturbations = robustness(explainer, X_test_df.head(10), n_perturbations=30, seed=SEED)\n",
        "    rb_higher_noise = robustness(explainer, X_test_df.head(10), noise_std=0.3, seed=SEED)\n",
        "\n",
        "    sufficiency_k1 = sufficiency(model, X_test, shap_vals, X.columns, k=1)\n",
        "    sufficiency_k3 = sufficiency(model, X_test, shap_vals, X.columns, k=3)\n",
        "    sufficiency_k5 = sufficiency(model, X_test, shap_vals, X.columns, k=5)\n",
        "    sufficiency_k8 = sufficiency(model, X_test, shap_vals, X.columns, k=8)\n",
        "\n",
        "    completeness_k1 = completeness(model, X_test, shap_vals, X.columns, k=1)\n",
        "    completeness_k3 = completeness(model, X_test, shap_vals, X.columns, k=3)\n",
        "    completeness_k5 = completeness(model, X_test, shap_vals, X.columns, k=5)\n",
        "    completeness_k8 = completeness(model, X_test, shap_vals, X.columns, k=8)\n",
        "\n",
        "    results[model_name] = {\n",
        "        'fidelity': fl_score,\n",
        "        'consistency': cs_score,\n",
        "        'dominant_feature': dominant_feature,\n",
        "        'dominant_percent': dominant_percent,\n",
        "        'robustness_baseline': rb_baseline,\n",
        "        'robustness_more_instances': rb_more_instances,\n",
        "        'robustness_more_perturbations': rb_more_perturbations,\n",
        "        'robustness_higher_noise': rb_higher_noise,\n",
        "        'sufficiency_k1': sufficiency_k1,\n",
        "        'sufficiency_k3': sufficiency_k3,\n",
        "        'sufficiency_k5': sufficiency_k5,\n",
        "        'sufficiency_k8': sufficiency_k8,\n",
        "        'completeness_k1': completeness_k1,\n",
        "        'completeness_k3': completeness_k3,\n",
        "        'completeness_k5': completeness_k5,\n",
        "        'completeness_k8': completeness_k8\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQcKERvSAwJV"
      },
      "source": [
        "##Results per Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-VXdLUrAxSU"
      },
      "outputs": [],
      "source": [
        "model_names = ['DT', 'EBM', 'CAT', 'LGBM']\n",
        "\n",
        "for model in model_names:\n",
        "    print(f\"\\nModel: {model}\")\n",
        "    for metric in ['fidelity', 'consistency', 'robustness_baseline', 'dominant_feature', 'dominant_percent']:\n",
        "        print(f\"{metric}: {results[model][metric]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3KG0LR1Ay4k"
      },
      "source": [
        "##Fidelity Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLrcCcalA0Kc"
      },
      "outputs": [],
      "source": [
        "fl_scores = [results[m]['fidelity'] for m in model_names]\n",
        "plt.bar(model_names, fl_scores)\n",
        "plt.ylabel('Correlation')\n",
        "plt.title('Fidelity')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWbik7U8A1q9"
      },
      "source": [
        "##Consistency Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjhweeLLA25t"
      },
      "outputs": [],
      "source": [
        "cs_scores = [results[m]['consistency'] for m in model_names]\n",
        "plt.bar(model_names, cs_scores)\n",
        "plt.ylabel('Correlation')\n",
        "plt.title('Consistency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSz6XpeHA4gh"
      },
      "source": [
        "##Robustness Testing Results for each Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OY-gLVeDA540"
      },
      "outputs": [],
      "source": [
        "for model in model_names:\n",
        "    print(f\"\\nModel: {model}\")\n",
        "    for metric in ['robustness_baseline', 'robustness_more_instances', 'robustness_more_perturbations', 'robustness_higher_noise']:\n",
        "        print(f\"{metric}: {results[model][metric]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6R5KRN9MA7UY"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "x = np.arange(len(model_names))\n",
        "width = 0.2\n",
        "\n",
        "rects1 = ax.bar(x - 1.5*width, [results[m]['robustness_baseline'] for m in model_names], width, label='RB 1')\n",
        "rects2 = ax.bar(x - 0.5*width, [results[m]['robustness_more_instances'] for m in model_names], width, label='RB 2')\n",
        "rects3 = ax.bar(x + 0.5*width, [results[m]['robustness_more_perturbations'] for m in model_names], width, label='RB 3')\n",
        "rects4 = ax.bar(x + 1.5*width, [results[m]['robustness_higher_noise'] for m in model_names], width, label='RB 4')\n",
        "\n",
        "ax.set_ylabel('Correlation')\n",
        "ax.set_title('Robustness by Model')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(model_names)\n",
        "ax.legend(['RB 1 (n_instances=10)', 'RB 2 (n_instances=30)', 'RB 3 (n_perturbations=30)', 'RB 4 (noise_std=0.3)'])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS3fOdhTA8_s"
      },
      "source": [
        "##Suffiency Testing Results for each Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YpykXjLA-Qh"
      },
      "outputs": [],
      "source": [
        "dt_top5 = set(results['DT']['sufficiency_k5']['feature_usage_df'].head(5)['feature'])\n",
        "ebm_top5 = set(results['EBM']['sufficiency_k5']['feature_usage_df'].head(5)['feature'])\n",
        "cat_top5 = set(results['CAT']['sufficiency_k5']['feature_usage_df'].head(5)['feature'])\n",
        "lgbm_top5 = set(results['LGBM']['sufficiency_k5']['feature_usage_df'].head(5)['feature'])\n",
        "\n",
        "print(f\"Decision Tree Top 5: {dt_top5}\")\n",
        "print(f\"EBM Top 5: {ebm_top5}\")\n",
        "print(f\"CatBoost Top 5: {cat_top5}\")\n",
        "print(f\"LightGBM Top 5: {lgbm_top5}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsDg7zPGA_3S"
      },
      "outputs": [],
      "source": [
        "percent_maintained_k1 = [results[m]['sufficiency_k1']['percent_maintained'] for m in model_names]\n",
        "percent_maintained_k3 = [results[m]['sufficiency_k3']['percent_maintained'] for m in model_names]\n",
        "percent_maintained_k5 = [results[m]['sufficiency_k5']['percent_maintained'] for m in model_names]\n",
        "percent_maintained_k8 = [results[m]['sufficiency_k8']['percent_maintained'] for m in model_names]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "x = np.arange(len(model_names))\n",
        "width = 0.2\n",
        "\n",
        "rects1 = ax.bar(x - 1.5*width, percent_maintained_k1, width, label='k=1', alpha=0.8)\n",
        "rects2 = ax.bar(x - 0.5*width, percent_maintained_k3, width, label='k=3', alpha=0.8)\n",
        "rects3 = ax.bar(x + 0.5*width, percent_maintained_k5, width, label='k=5', alpha=0.8)\n",
        "rects4 = ax.bar(x + 1.5*width, percent_maintained_k8, width, label='k=8', alpha=0.8)\n",
        "\n",
        "ax.set_ylabel('% Predictions Maintained (<10% error)', fontsize=11)\n",
        "ax.set_title('Sufficiency Test: Prediction Preservation for Different k', fontsize=12, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(model_names)\n",
        "ax.legend()\n",
        "ax.set_ylim([0, 1])\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVXjafq9hC5I"
      },
      "source": [
        "##Completeness Testing Results for each Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hM-EnQC8hGOz"
      },
      "outputs": [],
      "source": [
        "percent_maintained_k1 = [results[m]['completeness_k1']['percent_maintained'] for m in model_names]\n",
        "percent_maintained_k3 = [results[m]['completeness_k3']['percent_maintained'] for m in model_names]\n",
        "percent_maintained_k5 = [results[m]['completeness_k5']['percent_maintained'] for m in model_names]\n",
        "percent_maintained_k8 = [results[m]['completeness_k8']['percent_maintained'] for m in model_names]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "x = np.arange(len(model_names))\n",
        "width = 0.2\n",
        "\n",
        "rects1 = ax.bar(x - 1.5*width, percent_maintained_k1, width, label='k=1', alpha=0.8)\n",
        "rects2 = ax.bar(x - 0.5*width, percent_maintained_k3, width, label='k=3', alpha=0.8)\n",
        "rects3 = ax.bar(x + 0.5*width, percent_maintained_k5, width, label='k=5', alpha=0.8)\n",
        "rects4 = ax.bar(x + 1.5*width, percent_maintained_k8, width, label='k=8', alpha=0.8)\n",
        "\n",
        "ax.set_ylabel('% Predictions Maintained (<10% error)', fontsize=11)\n",
        "ax.set_title('Completeness Test: Prediction Preservation for Different k', fontsize=12, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(model_names)\n",
        "ax.legend()\n",
        "ax.set_ylim([0, 1])\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juhFNblSBBP9"
      },
      "source": [
        "##R2 Score vs Explainability Plot (Through mean values of metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb3iqwN4BCbC"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "explain_scores = {\n",
        "    m: np.mean([\n",
        "        results[m]['fidelity'],\n",
        "        results[m]['robustness_baseline'],\n",
        "        results[m]['consistency'],\n",
        "        results[m]['sufficiency_k5']['percent_maintained'],\n",
        "        results[m]['completeness_k3']['percent_maintained']\n",
        "    ]) for m in model_names\n",
        "}\n",
        "\n",
        "r2_scores = {\n",
        "    'DT': r2_score(y_test, dt_pred),\n",
        "    'EBM': r2_score(y_test, ebm_pred),\n",
        "    'CAT': r2_score(y_test, cat_pred),\n",
        "    'LGBM': r2_score(y_test, lgbm_pred)\n",
        "}\n",
        "\n",
        "for model in model_names:\n",
        "    ax.scatter(explain_scores[model], r2_scores[model])\n",
        "    ax.annotate(model, (explain_scores[model], r2_scores[model]))\n",
        "\n",
        "ax.set_xlabel('Explainability Score (0-1)')\n",
        "ax.set_ylabel('Model R2 Score')\n",
        "ax.set_title('R2 Score vs Explainability Tradeoff')\n",
        "ax.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oFG9kptBGT-"
      },
      "outputs": [],
      "source": [
        "for model in model_names:\n",
        "    print(f\"{model}, {explain_scores[model]}, {r2_scores[model]}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}