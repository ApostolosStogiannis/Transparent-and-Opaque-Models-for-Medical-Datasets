{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Setting Up the Models"
      ],
      "metadata": {
        "id": "SFTdtvrkvbm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Install Libraries and Import Packages"
      ],
      "metadata": {
        "id": "I89rASc-lsbr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bq7lPEqWjnzX"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements_cl.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import shap\n",
        "import graphviz\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "from scipy.stats import spearmanr\n",
        "from collections import Counter\n",
        "\n",
        "import random\n",
        "\n",
        "#fetch dataset\n",
        "heart_disease = fetch_ucirepo(id=45)\n",
        "\n",
        "#data\n",
        "df = heart_disease.data.original.copy()\n",
        "\n",
        "SEED = 100\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ],
      "metadata": {
        "id": "4BEVh87tlCvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Analysis"
      ],
      "metadata": {
        "id": "521EOsXJ6YlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "HuaQtKKP6ax0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "EfyFaeVO6cz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "gOffWkRm6fKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VoYF3a2j6jDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df.columns:\n",
        "    plt.boxplot(df[col])\n",
        "    plt.title(col)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "EEa-rEFJ6mP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "_fhhKuwV6pO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "66bSQtuC6rI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Handling NaN Values with SimpleImputer"
      ],
      "metadata": {
        "id": "-inT-s60m96C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imputer = SimpleImputer(missing_values = np.nan, strategy ='mean')\n",
        "imputer = imputer.fit(df)\n",
        "df = imputer.transform(df)\n",
        "\n",
        "#convert NP to DF\n",
        "df = pd.DataFrame(df, columns=heart_disease.data.original.columns)"
      ],
      "metadata": {
        "id": "6lqZucRtlfzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Combining Classes"
      ],
      "metadata": {
        "id": "I5zByFELnQ5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('num', axis=1)\n",
        "y = df['num']\n",
        "\n",
        "#for plot\n",
        "feature_names = X.columns\n",
        "\n",
        "#class 0 -> no disease(0) / classes 1-4 -> disease(1)\n",
        "y_binary = y.copy()\n",
        "y_binary[y_binary > 0] = 1"
      ],
      "metadata": {
        "id": "5TYRZMcMlihH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Spliting and Scaling the Data"
      ],
      "metadata": {
        "id": "tgHSeOk-nVFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.25, random_state=0, stratify=y_binary)\n",
        "\n",
        "#scale\n",
        "scaler = preprocessing.StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "IdsdqipelltV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating the Optimized Models and Then Fitting them"
      ],
      "metadata": {
        "id": "Dz8kJbFKreXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Decision Tree Model\n",
        "dt = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
        "                            max_depth=4, max_features=None, max_leaf_nodes=None,\n",
        "                            min_impurity_decrease=0.0, min_samples_leaf=16, min_samples_split=4,\n",
        "                            min_weight_fraction_leaf=0.0, monotonic_cst=None, random_state=100, splitter='random')\n",
        "\n",
        "#Logistic Regression Model\n",
        "lr = LogisticRegression(C=0.010993634452683504, class_weight='balanced', dual=False,\n",
        "                        fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
        "                        multi_class='deprecated', n_jobs=None, penalty='l2', random_state=100, solver='liblinear',\n",
        "                        tol=0.0001, verbose=0, warm_start=False)\n",
        "\n",
        "#Random Forest Model\n",
        "rf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
        "                            max_depth=15, max_features='sqrt', max_leaf_nodes=None, max_samples=None,\n",
        "                            min_impurity_decrease=0.0, min_samples_leaf=15, min_samples_split=16,\n",
        "                            min_weight_fraction_leaf=0.0, monotonic_cst=None, n_estimators=53, n_jobs=None,\n",
        "                            oob_score=False, random_state=100, verbose=0, warm_start=False)\n",
        "\n",
        "#XGBoost Model\n",
        "xgb = XGBClassifier(objective='binary:logistic', base_score=None, booster=None, callbacks=None,\n",
        "                    colsample_bylevel=None, colsample_bynode=None, colsample_bytree=0.7504717399139913,\n",
        "                    device=None, early_stopping_rounds=None, enable_categorical=False, eval_metric=None,\n",
        "                    feature_types=None, gamma=0.6497146261841261, grow_policy=None, importance_type=None,\n",
        "                    interaction_constraints=None, learning_rate=0.021618964126433812, max_bin=None, max_cat_threshold=None,\n",
        "                    max_cat_to_onehot=None, max_delta_step=None, max_depth=7, max_leaves=None, min_child_weight=None,\n",
        "                    missing=np.nan, monotone_constraints=None, multi_strategy=None, n_estimators=347, n_jobs=None,\n",
        "                    num_parallel_tree=None, random_state=100, reg_alpha=0.0010349590106072711, reg_lambda=0.02637083647277659,\n",
        "                    sampling_method=None, scale_pos_weight=8.443990241900503, subsample=0.7040523480851058, tree_method=None,\n",
        "                    validate_parameters=None, verbosity=None)"
      ],
      "metadata": {
        "id": "lwvykJqrrjsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt.fit(X_train, y_train)\n",
        "lr.fit(X_train, y_train)\n",
        "rf.fit(X_train, y_train)\n",
        "xgb.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "_lgnDenMrt6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluating Each Model's Performance"
      ],
      "metadata": {
        "id": "09DcC91nr2LB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For dt\n",
        "dt_pred = dt.predict(X_test)\n",
        "print(\"\\nFor the Decision Tree Model:\\nAccuracy: \" + str(accuracy_score(y_test, dt_pred) * 100) + \"%\")\n",
        "print(\"Classification Report:\\n\" + str(classification_report(y_test, dt_pred)))"
      ],
      "metadata": {
        "id": "Q5qq4ahbr66U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For lr\n",
        "lr_pred = lr.predict(X_test)\n",
        "print(\"\\nFor the Logistic Regression Model:\\nAccuracy: \" + str(accuracy_score(y_test, lr_pred) * 100) + \"%\")\n",
        "print(\"Classification Report:\\n\" + str(classification_report(y_test, lr_pred)))"
      ],
      "metadata": {
        "id": "N-yKI6o4tWMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For rf\n",
        "rf_pred = rf.predict(X_test)\n",
        "print(\"\\nFor the Random Forest Model:\\nAccuracy: \" + str(accuracy_score(y_test, rf_pred) * 100) + \"%\")\n",
        "print(\"Classification Report:\\n\" + str(classification_report(y_test, rf_pred)))"
      ],
      "metadata": {
        "id": "Jvg7zZTbtbw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For xgb\n",
        "xgb_pred = xgb.predict(X_test)\n",
        "print(\"\\nFor the XGBoost Model:\\nAccuracy: \" + str(accuracy_score(y_test, xgb_pred) * 100) + \"%\")\n",
        "print(\"Classification Report:\\n\" + str(classification_report(y_test, xgb_pred)))"
      ],
      "metadata": {
        "id": "Af9Zv41ottmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Global Explainability"
      ],
      "metadata": {
        "id": "QwM0_xQXvgeu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Decision Tree's Tree Structure"
      ],
      "metadata": {
        "id": "8yzHE6u0vuYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dot_data = export_graphviz(dt, out_file=None,\n",
        "                                feature_names=feature_names,\n",
        "                                class_names=[str(x) for x in y_binary.unique()],\n",
        "                                filled=True)\n",
        "\n",
        "graph = graphviz.Source(dot_data, format=\"png\")\n",
        "graph"
      ],
      "metadata": {
        "id": "hqcxwtBbvvtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Logistic Regression Model's Coefficients"
      ],
      "metadata": {
        "id": "G0BKojeTwiPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coefficients = pd.Series(lr.coef_[0], index=feature_names)\n",
        "print(\"Logistic Regression Coefficients:\\n\")\n",
        "print(coefficients.sort_values(ascending=False))"
      ],
      "metadata": {
        "id": "GlBKNUjfwx1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coefficients.sort_values().plot(kind=\"barh\", figsize=(8, 6))\n",
        "plt.title(\"Logistic Regression Coefficients\")\n",
        "plt.xlabel(\"Coefficient Value\")\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_8zJ7D0Qw3RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setting Up SHAP explainers"
      ],
      "metadata": {
        "id": "45oSUf4Qw8gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#turn data into DF for SHAP plots\n",
        "X_test_df = pd.DataFrame(X_test, columns=feature_names)\n",
        "\n",
        "#SHAP for dt\n",
        "dt_explainer = shap.TreeExplainer(dt)\n",
        "dt_shap_values = dt_explainer.shap_values(X_test_df)\n",
        "\n",
        "#SHAP for lr\n",
        "lr_explainer = shap.LinearExplainer(lr, masker=shap.maskers.Independent(X_test_df))\n",
        "lr_shap_values = lr_explainer.shap_values(X_test_df)\n",
        "\n",
        "#SHAP for rf\n",
        "rf_explainer = shap.TreeExplainer(rf)\n",
        "rf_shap_values = rf_explainer.shap_values(X_test_df)\n",
        "\n",
        "#SHAP for xgb\n",
        "xgb_explainer = shap.TreeExplainer(xgb, X_train, feature_perturbation='interventional')\n",
        "xgb_shap_values = xgb_explainer.shap_values(X_test_df)"
      ],
      "metadata": {
        "id": "4O3ecEmlwe0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SHAP Summary Plots For Each Model"
      ],
      "metadata": {
        "id": "ohoS4poSymkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt_shap_values.shape"
      ],
      "metadata": {
        "id": "7UqL6k497C9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dt\n",
        "shap.summary_plot(dt_shap_values[:, :, 1], X_test_df)"
      ],
      "metadata": {
        "id": "gh0CDQCG-PrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_shap_values.shape"
      ],
      "metadata": {
        "id": "enTK74YV7IZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lr\n",
        "shap.summary_plot(lr_shap_values, X_test_df)"
      ],
      "metadata": {
        "id": "8vgdvjKv-m_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_shap_values.shape"
      ],
      "metadata": {
        "id": "JgQmWykk7RuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rf\n",
        "shap.summary_plot(rf_shap_values[:, :, 1], X_test_df)"
      ],
      "metadata": {
        "id": "XXUfxy3H-ttD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_shap_values.shape"
      ],
      "metadata": {
        "id": "5ItsKZ4S7Wta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#xgb\n",
        "shap.summary_plot(xgb_shap_values, X_test_df)"
      ],
      "metadata": {
        "id": "uGKirkMa-ztv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Local Explainability"
      ],
      "metadata": {
        "id": "Q87RLY7f_Pcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Selecting Instance"
      ],
      "metadata": {
        "id": "SWA6plKA_ZSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = 0"
      ],
      "metadata": {
        "id": "6IqjWW_y_Yah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Decision Path"
      ],
      "metadata": {
        "id": "EQHxkMZy_dEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_instance = X_test[index].reshape(1, -1)\n",
        "\n",
        "node_indicator = dt.decision_path(X_instance)\n",
        "leaf_id = dt.apply(X_instance)\n",
        "\n",
        "print(f\"\\nDecision path for instance {index}:\")\n",
        "for node_id in node_indicator.indices:\n",
        "    if dt.tree_.children_left[node_id] != dt.tree_.children_right[node_id]:\n",
        "        feature = feature_names[dt.tree_.feature[node_id]]\n",
        "        threshold = dt.tree_.threshold[node_id]\n",
        "        if X_instance[0, dt.tree_.feature[node_id]] <= threshold:\n",
        "            threshold_sign = \"<=\"\n",
        "        else:\n",
        "            threshold_sign = \">\"\n",
        "        print(f\"  {feature} = {X_instance[0, dt.tree_.feature[node_id]]:.2f} \"\n",
        "              f\"{threshold_sign} {threshold:.2f}\")\n",
        "\n",
        "pred_class = dt.predict(X_instance)[0]\n",
        "true_class = y_test.iloc[index] if isinstance(y_test, pd.Series) else y_test[index]\n",
        "\n",
        "print(f\"\\nPredicted class: {pred_class}\")\n",
        "print(f\"Actual class:    {true_class}\")"
      ],
      "metadata": {
        "id": "v8vBY_87_ccZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Logistic Regression's Contributions For Single Instance"
      ],
      "metadata": {
        "id": "3iGhrlojAO6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contributions = X_test_df.iloc[index] * lr.coef_[0]\n",
        "print(contributions.sort_values(ascending=False))"
      ],
      "metadata": {
        "id": "7lhmOieiAVL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SHAP Waterfalls"
      ],
      "metadata": {
        "id": "tISaiMu5_yE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dt\n",
        "shap.initjs()\n",
        "shap.force_plot(dt_explainer.expected_value[1], dt_shap_values[index, :, 1], X_test_df.iloc[index])"
      ],
      "metadata": {
        "id": "GJKmv5u7_xSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lr\n",
        "shap.initjs()\n",
        "shap.force_plot(lr_explainer.expected_value, lr_shap_values[index, :], X_test_df.iloc[index])"
      ],
      "metadata": {
        "id": "dSgpuc98A9Ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rf\n",
        "shap.initjs()\n",
        "shap.force_plot(rf_explainer.expected_value[1], rf_shap_values[index, :, 1], X_test_df.iloc[index])"
      ],
      "metadata": {
        "id": "qUmfTvcPBCWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#xgb\n",
        "shap.initjs()\n",
        "shap.force_plot(xgb_explainer.expected_value, xgb_shap_values[index, :], X_test_df.iloc[index])"
      ],
      "metadata": {
        "id": "va5tfNqDBe1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SHAP Evaluation"
      ],
      "metadata": {
        "id": "kO2NUkCdaA6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models_dict = {\n",
        "    'DT': (dt, dt_explainer, dt_shap_values, dt.feature_importances_),\n",
        "    'LR': (lr, lr_explainer, lr_shap_values, np.abs(lr.coef_[0])),\n",
        "    'RF': (rf, rf_explainer, rf_shap_values, rf.feature_importances_),\n",
        "    'XGB': (xgb, xgb_explainer, xgb_shap_values, xgb.feature_importances_)\n",
        "}"
      ],
      "metadata": {
        "id": "ZKyfAdk47bU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fidelity (Correlation between model and SHAP feature importances)"
      ],
      "metadata": {
        "id": "X6_RUicpuiMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fidelity(model_importance, shap_values):\n",
        "    if len(shap_values.shape) == 3:\n",
        "        shap_importance = np.abs(shap_values).mean(axis=(0, 2))\n",
        "    else:\n",
        "        shap_importance = np.abs(shap_values).mean(axis=0)\n",
        "    return spearmanr(model_importance, shap_importance)[0]"
      ],
      "metadata": {
        "id": "_-5mjA0bXLT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Consistency (Entropy of top feature across instances)"
      ],
      "metadata": {
        "id": "JW_hLBk6Q6hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def consistency(shap_values, feature_names):\n",
        "    if len(shap_values.shape) == 3:\n",
        "        top_feature = np.argmax(np.abs(shap_values[:, :, 1]), axis=1)\n",
        "    else:\n",
        "        top_feature = np.argmax(np.abs(shap_values), axis=1)\n",
        "\n",
        "    value, counts = np.unique(top_feature, return_counts=True)\n",
        "    probs = counts / len(top_feature)\n",
        "    entropy = -np.sum(probs * np.log(probs))\n",
        "\n",
        "    dominant_feature = feature_names[value[np.argmax(counts)]]\n",
        "    dominant_percent = counts.max() / len(top_feature)\n",
        "\n",
        "    return entropy, dominant_feature, dominant_percent"
      ],
      "metadata": {
        "id": "hPug5PxEQ7Kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Robustness (Test if SHAP values remain stable under small perturbations)"
      ],
      "metadata": {
        "id": "DRhKqNdzViNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def robustness(explainer, X_sample_df, n_instances=10, n_perturbations=10, noise_std=0.1, seed=100):\n",
        "    np.random.seed(seed)\n",
        "    stabilities = []\n",
        "\n",
        "    for i in range(min(n_instances, len(X_sample_df))):\n",
        "        instance_df = X_sample_df.iloc[i:i+1]\n",
        "        base_shap = explainer.shap_values(instance_df)\n",
        "\n",
        "        corrs = []\n",
        "        for _ in range(n_perturbations):\n",
        "            noise = np.random.normal(0, noise_std, instance_df.shape)\n",
        "            perturbed_df = pd.DataFrame(instance_df.values + noise, columns=instance_df.columns)\n",
        "            perturbed_shap = explainer.shap_values(perturbed_df)\n",
        "            corr = spearmanr(base_shap.flatten(), perturbed_shap.flatten())[0]\n",
        "            corrs.append(corr)\n",
        "\n",
        "        stabilities.append(np.mean(corrs))\n",
        "\n",
        "    return np.mean(stabilities)"
      ],
      "metadata": {
        "id": "XK2KRdtIVe2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sufficiency (Test if top-k SHAP features preserve predictions)"
      ],
      "metadata": {
        "id": "pNNh4ZGqWyx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sufficiency(model, X_test, shap_values, feature_names, k=5, n_samples=30):\n",
        "    prob_diffs = []\n",
        "    class_maintained = []\n",
        "    all_top_features = []\n",
        "\n",
        "    for i in range(min(n_samples, len(X_test))):\n",
        "        original_pred = model.predict_proba(X_test[i].reshape(1, -1))[0]\n",
        "        original_class = np.argmax(original_pred)\n",
        "\n",
        "        if len(shap_values.shape) == 3:\n",
        "            shap_vals = shap_values[i, :, 1]\n",
        "        else:\n",
        "            shap_vals = shap_values[i, :]\n",
        "\n",
        "        top_k_indices = np.argsort(np.abs(shap_vals))[-k:]\n",
        "        all_top_features.extend(top_k_indices)\n",
        "\n",
        "        masked_instance = np.zeros_like(X_test[i])\n",
        "        masked_instance[top_k_indices] = X_test[i][top_k_indices]\n",
        "\n",
        "        masked_pred = model.predict_proba(masked_instance.reshape(1, -1))[0]\n",
        "        masked_class = np.argmax(masked_pred)\n",
        "\n",
        "        class_maintained.append(original_class == masked_class)\n",
        "\n",
        "    feature_counter = Counter(all_top_features)\n",
        "\n",
        "    feature_usage = []\n",
        "    for feat_idx in range(len(feature_names)):\n",
        "        count = feature_counter.get(feat_idx, 0)\n",
        "        percent = (count / n_samples) * 100\n",
        "        feature_usage.append({'feature': feature_names[feat_idx], 'count': count, 'percentage': percent})\n",
        "\n",
        "    feature_usage_df = pd.DataFrame(feature_usage).sort_values('count', ascending=False)\n",
        "\n",
        "    top_features_dict = {\n",
        "        feature_names[idx]: count\n",
        "        for idx, count in feature_counter.most_common(k)}\n",
        "\n",
        "    return {\n",
        "        'percent_class_maintained': np.mean(class_maintained),\n",
        "        'top_features_used': top_features_dict,\n",
        "        'feature_usage_df': feature_usage_df\n",
        "    }"
      ],
      "metadata": {
        "id": "hq4uYDy7ZUjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Completeness (Test if top-k SHAP features removals preserve predictions)"
      ],
      "metadata": {
        "id": "NBmkX0LqPm_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def completeness(model, X_test, shap_values, feature_names, k=5, n_samples=30):\n",
        "    prob_diffs = []\n",
        "    class_maintained = []\n",
        "    all_removed_features = []\n",
        "\n",
        "    for i in range(min(n_samples, len(X_test))):\n",
        "        original_pred = model.predict_proba(X_test[i].reshape(1, -1))[0]\n",
        "        original_class = np.argmax(original_pred)\n",
        "\n",
        "        if len(shap_values.shape) == 3:\n",
        "            shap_vals = shap_values[i, :, 1]\n",
        "        else:\n",
        "            shap_vals = shap_values[i, :]\n",
        "\n",
        "        top_k_indices = np.argsort(np.abs(shap_vals))[-k:]\n",
        "        all_removed_features.extend(top_k_indices)\n",
        "\n",
        "        masked_instance = np.copy(X_test[i])\n",
        "        masked_instance[top_k_indices] = 0\n",
        "\n",
        "        masked_pred = model.predict_proba(masked_instance.reshape(1, -1))[0]\n",
        "        masked_class = np.argmax(masked_pred)\n",
        "\n",
        "        class_maintained.append(original_class == masked_class)\n",
        "\n",
        "    feature_counter = Counter(all_removed_features)\n",
        "\n",
        "    feature_usage = []\n",
        "    for feat_idx in range(len(feature_names)):\n",
        "        count = feature_counter.get(feat_idx, 0)\n",
        "        percent = (count / n_samples) * 100\n",
        "        feature_usage.append({'feature': feature_names[feat_idx], 'count': count, 'percentage': percent})\n",
        "\n",
        "    feature_usage_df = pd.DataFrame(feature_usage).sort_values('count', ascending=False)\n",
        "\n",
        "    top_features_dict = {\n",
        "        feature_names[idx]: count\n",
        "        for idx, count in feature_counter.most_common(k)}\n",
        "\n",
        "    return {\n",
        "        'percent_class_maintained': np.mean(class_maintained),\n",
        "        'top_features_removed': top_features_dict,\n",
        "        'feature_usage_df': feature_usage_df\n",
        "    }"
      ],
      "metadata": {
        "id": "txhCecXEPzGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SHAP Evaluation Results"
      ],
      "metadata": {
        "id": "g2Mj0lPiSE-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "\n",
        "for model_name, (model, explainer, shap_vals, feat_imp) in models_dict.items():\n",
        "    fl_score = fidelity(feat_imp, shap_vals)\n",
        "\n",
        "    cs_entropy, dominant_feature, dominant_percent = consistency(shap_vals, feature_names)\n",
        "    cs_score = 1 - (cs_entropy / np.log(len(feature_names)))#min-max normalization\n",
        "\n",
        "    rb_baseline = robustness(explainer, X_test_df.head(10), seed=SEED)\n",
        "    rb_more_instances = robustness(explainer, X_test_df.head(30), n_instances=30, seed=SEED)\n",
        "    rb_more_perturbations = robustness(explainer, X_test_df.head(10), n_perturbations=30, seed=SEED)\n",
        "    rb_higher_noise = robustness(explainer, X_test_df.head(10), noise_std=0.3, seed=SEED)\n",
        "\n",
        "    sufficiency_k1 = sufficiency(model, X_test, shap_vals, feature_names, k=1)\n",
        "    sufficiency_k3 = sufficiency(model, X_test, shap_vals, feature_names, k=3)\n",
        "    sufficiency_k5 = sufficiency(model, X_test, shap_vals, feature_names, k=5)\n",
        "    sufficiency_k8 = sufficiency(model, X_test, shap_vals, feature_names, k=8)\n",
        "\n",
        "    completeness_k1 = completeness(model, X_test, shap_vals, feature_names, k=1)\n",
        "    completeness_k3 = completeness(model, X_test, shap_vals, feature_names, k=3)\n",
        "    completeness_k5 = completeness(model, X_test, shap_vals, feature_names, k=5)\n",
        "    completeness_k8 = completeness(model, X_test, shap_vals, feature_names, k=8)\n",
        "\n",
        "\n",
        "    results[model_name] = {\n",
        "        'fidelity': fl_score,\n",
        "        'consistency': cs_score,\n",
        "        'dominant_feature': dominant_feature,\n",
        "        'dominant_percent': dominant_percent,\n",
        "        'robustness_baseline': rb_baseline,\n",
        "        'robustness_more_instances': rb_more_instances,\n",
        "        'robustness_more_perturbations': rb_more_perturbations,\n",
        "        'robustness_higher_noise': rb_higher_noise,\n",
        "        'sufficiency_k1': sufficiency_k1,\n",
        "        'sufficiency_k3': sufficiency_k3,\n",
        "        'sufficiency_k5': sufficiency_k5,\n",
        "        'sufficiency_k8': sufficiency_k8,\n",
        "        'completeness_k1': completeness_k1,\n",
        "        'completeness_k3': completeness_k3,\n",
        "        'completeness_k5': completeness_k5,\n",
        "        'completeness_k8': completeness_k8\n",
        "    }"
      ],
      "metadata": {
        "id": "7qoV7kFrSDxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Results per Model"
      ],
      "metadata": {
        "id": "B8SKpYyiSNwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_names = ['DT', 'LR', 'RF', 'XGB']\n",
        "\n",
        "for model in model_names:\n",
        "    print(f\"\\nModel: {model}\")\n",
        "    for metric in ['fidelity', 'consistency', 'robustness_baseline', 'dominant_feature', 'dominant_percent']:\n",
        "        print(f\"{metric}: {results[model][metric]}\")"
      ],
      "metadata": {
        "id": "WhmUf_UsSL3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fidelity Plot"
      ],
      "metadata": {
        "id": "8r1v-vBEyR8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fl_scores = [results[m]['fidelity'] for m in model_names]\n",
        "plt.bar(model_names, fl_scores)\n",
        "plt.ylabel('Correlation')\n",
        "plt.title('Fidelity')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BotPqNkCvCGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Consistency Plot"
      ],
      "metadata": {
        "id": "9KsDpnnwynCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cs_scores = [results[m]['consistency'] for m in model_names]\n",
        "plt.bar(model_names, cs_scores)\n",
        "plt.ylabel('Correlation')\n",
        "plt.title('Consistency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0PvmhxsMvP9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Robustness Testing Results for each Model"
      ],
      "metadata": {
        "id": "r9OWX_qCysr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model in model_names:\n",
        "    print(f\"\\nModel: {model}\")\n",
        "    for metric in ['robustness_baseline', 'robustness_more_instances', 'robustness_more_perturbations', 'robustness_higher_noise']:\n",
        "        print(f\"{metric}: {results[model][metric]}\")"
      ],
      "metadata": {
        "id": "fanld_vD55rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "x = np.arange(len(model_names))\n",
        "width = 0.2\n",
        "\n",
        "rects1 = ax.bar(x - 1.5*width, [results[m]['robustness_baseline'] for m in model_names], width, label='RB 1')\n",
        "rects2 = ax.bar(x - 0.5*width, [results[m]['robustness_more_instances'] for m in model_names], width, label='RB 2')\n",
        "rects3 = ax.bar(x + 0.5*width, [results[m]['robustness_more_perturbations'] for m in model_names], width, label='RB 3')\n",
        "rects4 = ax.bar(x + 1.5*width, [results[m]['robustness_higher_noise'] for m in model_names], width, label='RB 4')\n",
        "\n",
        "ax.set_ylabel('Correlation')\n",
        "ax.set_title('Robustness by Model')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(model_names)\n",
        "ax.legend(['RB 1 (n_instances=10)', 'RB 2 (n_instances=30)', 'RB 3 (n_perturbations=30)', 'RB 4 (noise_std=0.3)'])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8yAttMCM5YBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Suffiency Testing Results for each Model"
      ],
      "metadata": {
        "id": "np24pWFrzEJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt_top5 = set(results['DT']['sufficiency_k5']['feature_usage_df'].head(5)['feature'])\n",
        "lr_top5 = set(results['LR']['sufficiency_k5']['feature_usage_df'].head(5)['feature'])\n",
        "rf_top5 = set(results['RF']['sufficiency_k5']['feature_usage_df'].head(5)['feature'])\n",
        "xgb_top5 = set(results['XGB']['sufficiency_k5']['feature_usage_df'].head(5)['feature'])\n",
        "\n",
        "print(f\"Decision Tree Top 5 Features: {dt_top5}\")\n",
        "print(f\"Logistic Regression Top 5 Features: {lr_top5}\")\n",
        "print(f\"Random Forest Top 5 Features: {rf_top5}\")\n",
        "print(f\"XGBoost Top 5 Features: {xgb_top5}\")"
      ],
      "metadata": {
        "id": "lVYBQWX21lnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "percent_maintained_k1 = [results[m]['sufficiency_k1']['percent_class_maintained'] for m in model_names]\n",
        "percent_maintained_k3 = [results[m]['sufficiency_k3']['percent_class_maintained'] for m in model_names]\n",
        "percent_maintained_k5 = [results[m]['sufficiency_k5']['percent_class_maintained'] for m in model_names]\n",
        "percent_maintained_k8 = [results[m]['sufficiency_k8']['percent_class_maintained'] for m in model_names]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "x = np.arange(len(model_names))\n",
        "width = 0.2\n",
        "\n",
        "rects1 = ax.bar(x - 1.5*width, percent_maintained_k1, width, label='k=1')\n",
        "rects2 = ax.bar(x - 0.5*width, percent_maintained_k3, width, label='k=3')\n",
        "rects3 = ax.bar(x + 0.5*width, percent_maintained_k5, width, label='k=5')\n",
        "rects4 = ax.bar(x + 1.5*width, percent_maintained_k8, width, label='k=8')\n",
        "\n",
        "ax.set_ylabel('Percent Class Maintained')\n",
        "ax.set_title('Sufficiency Test: Percent Class Maintained for different k')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(model_names)\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TlFZ_eIwCwJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k_values = [1, 3, 5, 8]\n",
        "\n",
        "for k in k_values:\n",
        "    print(f\"Sufficiency Test: Top Features Used for k={k}\")\n",
        "    for model_name in model_names:\n",
        "        sufficiency_results = results[model_name][f'sufficiency_k{k}']\n",
        "        feature_usage_df = sufficiency_results['feature_usage_df']\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.bar(feature_usage_df['feature'], feature_usage_df['percentage'])\n",
        "        plt.ylabel('Percentage of Instances')\n",
        "        plt.xlabel('Feature')\n",
        "        plt.title(f\"Top Features Used in Sufficiency Test (k={k}) for {model_name}\")\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "zt8y2GRaHJ9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Completeness Testing Results for each Model"
      ],
      "metadata": {
        "id": "SsY9rFrysXd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "percent_maintained_k1 = [results[m]['completeness_k1']['percent_class_maintained'] for m in model_names]\n",
        "percent_maintained_k3 = [results[m]['completeness_k3']['percent_class_maintained'] for m in model_names]\n",
        "percent_maintained_k5 = [results[m]['completeness_k5']['percent_class_maintained'] for m in model_names]\n",
        "percent_maintained_k8 = [results[m]['completeness_k8']['percent_class_maintained'] for m in model_names]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "x = np.arange(len(model_names))\n",
        "width = 0.2\n",
        "\n",
        "rects1 = ax.bar(x - 1.5*width, percent_maintained_k1, width, label='k=1')\n",
        "rects2 = ax.bar(x - 0.5*width, percent_maintained_k3, width, label='k=3')\n",
        "rects3 = ax.bar(x + 0.5*width, percent_maintained_k5, width, label='k=5')\n",
        "rects4 = ax.bar(x + 1.5*width, percent_maintained_k8, width, label='k=8')\n",
        "\n",
        "ax.set_ylabel('Percent Class Maintained')\n",
        "ax.set_title('Completeness Test: Percent Class Maintained for different k')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(model_names)\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Eb_kRAF6uuiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in k_values:\n",
        "    print(f\"Completeness Test: Top Features Used for k={k}\")\n",
        "    for model_name in model_names:\n",
        "        completeness_results = results[model_name][f'completeness_k{k}']\n",
        "        feature_usage_df = completeness_results['feature_usage_df']\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.bar(feature_usage_df['feature'], feature_usage_df['percentage'])\n",
        "        plt.ylabel('Percentage of Instances')\n",
        "        plt.xlabel('Feature')\n",
        "        plt.title(f\"Top Features Used in Completeness Test (k={k}) for {model_name}\")\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "_vtOnDUOvMvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Accuracy vs Explainability Plot (Through mean values of metrics)"
      ],
      "metadata": {
        "id": "vVHvFjyjzM2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "explain_scores = {\n",
        "    m: np.mean([\n",
        "        results[m]['fidelity'],\n",
        "        results[m]['consistency'],\n",
        "        results[m]['robustness_baseline'],\n",
        "        results[m]['sufficiency_k5']['percent_class_maintained'],\n",
        "        results[m]['completeness_k3']['percent_class_maintained']\n",
        "    ]) for m in model_names\n",
        "}\n",
        "\n",
        "accuracies = {\n",
        "    'DT': accuracy_score(y_test, dt_pred),\n",
        "    'LR': accuracy_score(y_test, lr_pred),\n",
        "    'RF': accuracy_score(y_test, rf_pred),\n",
        "    'XGB': accuracy_score(y_test, xgb_pred)\n",
        "}\n",
        "\n",
        "for model in model_names:\n",
        "    ax.scatter(explain_scores[model], accuracies[model])\n",
        "    ax.annotate(model, (explain_scores[model], accuracies[model]))\n",
        "\n",
        "ax.set_xlabel('Explainability Score (0-1)')\n",
        "ax.set_ylabel('Model Accuracy')\n",
        "ax.set_title('Accuracy vs Explainability Tradeoff')\n",
        "ax.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TsT36HkLxVDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model in model_names:\n",
        "    print(f\"{model}, {explain_scores[model]}, {accuracies[model]}\")"
      ],
      "metadata": {
        "id": "T3qpdfc705mi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}